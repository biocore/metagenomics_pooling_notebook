{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu, zscore\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from contextlib import suppress\n",
    "from metapool.metapool import *\n",
    "from metapool import (make_sample_sheet, requires_dilution, dilute_gDNA,\n",
    "                      find_threshold, autopool, extract_stats_metadata)\n",
    "%watermark -i -v -iv -m -h -p metapool,sample_sheet,openpyxl -u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knight Lab shotgun pipeline notebook\n",
    "\n",
    "### What is it?\n",
    "\n",
    "This Jupyter Notebook allows you to automatically produce most of the files you need for completing the Knight Lab shotgun sequencing pipeline.\n",
    "\n",
    "Hopefully, this will not only make it much easier to generate these files, but also keep our information more accurate.\n",
    "\n",
    "### Here's how it should work.\n",
    "\n",
    "You'll start out with a **basic plate map**, which just links each sample to its appropriate well location, and tracks sample extraction metadata.\n",
    "\n",
    "Then you'll add the output of the MiniPico assay of sample DNA concentrations, which will enable to you to automatically make a **normalization pick list** for starting the shotgun library prep itself. You can also visualize these concentrations on the plate, allowing you to double check the plate map and DNA concentration read.\n",
    "\n",
    "Next you'll automatically assign barcodes to each sample, producing an **index pick list** for barcode addition prior to PCR.\n",
    "\n",
    "After finishing the shotgun library prep itself, you'll measure library concentration with qPCR. This qPCR data file will then be read in and used to estimate and visualize pooling parameters, producing a **pooling pick list**. \n",
    "\n",
    "Finally, the per-sample information from the whole run can be combined to automatically produce a **sample sheet** that you can give directly to IGM for sequencing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for normalizing DNA\n",
    "\n",
    "This portion of the notebook will read in the output of the mini-Pico quantification assay and construct an Echo normalization picklist file. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A tab-delimited row-wise plate map that indicates the sample name, well location, and blank status of each sample on the 384 well plate.\n",
    "\n",
    "You can use this google sheet template to generate your plate map:\n",
    "https://docs.google.com/spreadsheets/d/1xPjB6iR3brGeG4bm2un4ISSsTDxFw5yME09bKqz0XNk/edit?usp=sharing\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the plate map and constructs a dataframe\n",
    "2. calculates volumes to be added via echo to reach desired input DNA quantity\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: read in plate map\n",
    "\n",
    "**Enter the correct path to the plate map file**. This will serve as the plate map for relating all subsequent information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_map_fp = './test_data/Plate_Maps/2022_summer_Celeste_Adaptation_16_17_18_21_plate_map.tsv'\n",
    "\n",
    "if not os.path.isfile(plate_map_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the plate map**. It should look something like this:\n",
    "\n",
    "```\n",
    "Sample\tRow\tCol\tBlank\n",
    "GLY_01_012\tA\t1\tFalse\n",
    "GLY_14_034\tB\t1\tFalse\n",
    "GLY_11_007\tC\t1\tFalse\n",
    "GLY_28_018\tD\t1\tFalse\n",
    "GLY_25_003\tE\t1\tFalse\n",
    "GLY_06_106\tF\t1\tFalse\n",
    "GLY_07_011\tG\t1\tFalse\n",
    "GLY_18_043\tH\t1\tFalse\n",
    "GLY_28_004\tI\t1\tFalse\n",
    "```\n",
    "**Make sure there a no duplicate IDs**. If each sample doesn't have a different name, an **error** will be thrown and you won't be able to generate a sample sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to specify the directory for qiita.oauth.cfg\n",
    "plate_df = read_plate_map_csv(plate_map_fp)\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: read in DNA concentrations and add to plate map\n",
    "\n",
    "**Enter the correct path to the Pico DNA concentration output**. This should be a csv-formatted file produced by the MiniPico assay on the condensed, 384-well plate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_concs_fp = './test_data/Quant/MiniPico/2022_07_Celeste_Adaptation_16_17_18_21_gDNA_quant.txt'\n",
    "\n",
    "if not os.path.isfile(sample_concs_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the DNA concentration output file**. It should look something like this:\n",
    "    \n",
    "```\n",
    "Results\n",
    "\n",
    "Well ID\tWell\t[Blanked-RFU]\t[Concentration]\n",
    "SPL1\tA1\t5243.000\t3.432\n",
    "SPL2\tC1\t4949.000\t3.239\n",
    "SPL3\tE1\t15302.000\t10.016\n",
    "SPL4\tG1\t4039.000\t2.644\n",
    "SPL5\tI1\t12862.000\t8.419\n",
    "SPL6\tK1\t2840.000\t1.859\n",
    "SPL7\tM1\t3343.000\t2.188\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_concs = read_pico_csv(sample_concs_fp, plate_reader='SpectraMax_i3x')\n",
    "\n",
    "plate_df = pd.merge(plate_df, sample_concs, on='Well')\n",
    "\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if requires_dilution(plate_df,threshold=10,tolerance=.10):\n",
    "    plate_df = dilute_gDNA(plate_df,threshold=10)\n",
    "    print('You need to make a 1:10 gDNA dilution plate.')\n",
    "else:\n",
    "    plate_df['extracted_gdna_concentration_ng_ul'] = plate_df['Sample DNA Concentration'].copy()\n",
    "    plate_df['Diluted'] = False\n",
    "    print('Proceed w/out gDNA dilutions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize plate DNA concentrations and plate map:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name for sample well identifier\n",
    "well_col = 'Well'\n",
    "\n",
    "# get DNA concentratin information\n",
    "dna_concs = make_2D_array(plate_df, data_col='Sample DNA Concentration', well_col=well_col).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "names = make_2D_array(plate_df, data_col='Sample', well_col=well_col)\n",
    "\n",
    "plot_plate_vals(dna_concs,\n",
    "                annot_str=names,\n",
    "                color_map='viridis',\n",
    "                annot_fmt='.5s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sample replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of replicates\n",
    "\n",
    "# replicate_dict = {source1_quadrant:destination1_quadrant}\n",
    "# replicate_dict = {source1_quadrant:[destination1_quadrants,destination1_quadrants]}\n",
    "\n",
    "#replicate_dict = {1:[2,3]}\n",
    "\n",
    "# initialize a new PlateReplication object to manage metadata, conversions, and more for you.\n",
    "# initialize w/preferred well_col.\n",
    "well_col = 'Library Well'\n",
    "# 'Library Well' differs from 'Well' because the later specifies the gDNA source well while the former\n",
    "# specifies the well (destination well) that will contain the sequencing library for the sample.\n",
    "pr = PlateReplication(well_col)\n",
    "\n",
    "# set overwrite=False to detect any overwriting of source or destination quads and raise an Error.\n",
    "# replace replicates = None with replicates = replicate_dict to make replicates\n",
    "plate_df = pr.make_replicates(plate_df, replicates=None, overwrite=True)\n",
    "\n",
    "#replicates overlapping sample_wells for other samples shuld raise warning, but will be allowed\n",
    "if 'True' in plate_df['contains_replicates'].unique():\n",
    "    plate_df['contains_replicates'] = True\n",
    "    # get DNA concentratin information\n",
    "    dna_concs = make_2D_array(plate_df, data_col='Sample DNA Concentration', well_col=well_col).astype(float)\n",
    "\n",
    "    # get information for annotation\n",
    "    names = make_2D_array(plate_df, data_col='Sample', well_col=well_col)\n",
    "\n",
    "    plot_plate_vals(dna_concs,\n",
    "                annot_str=names,\n",
    "                color_map='viridis',\n",
    "                annot_fmt='.6s')\n",
    "else:\n",
    "    plate_df['contains_replicates'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make mask arrays for even and odd rows and columns\n",
    "\n",
    "even_rows = [x for x in range(16) if x % 2 == 0]\n",
    "odd_rows = [x for x in range(16) if x % 2 == 1]\n",
    "even_cols = [x for x in range(24) if x % 2 == 0]\n",
    "odd_cols = [x for x in range(24) if x % 2 == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gDNA concentration heatmap, Plate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(even_rows,even_cols)],\n",
    "                annot_str= names[np.ix_(even_rows,even_cols)],\n",
    "                color_map='viridis',\n",
    "                annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gDNA concentration heatmap, Plate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(even_rows,odd_cols)],\n",
    "                    annot_str= names[np.ix_(even_rows,odd_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gDNA concentration heatmap, Plate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(odd_rows,even_cols)],\n",
    "                    annot_str= names[np.ix_(odd_rows,even_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gDNA concentration heatmap, Plate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(odd_rows,odd_cols)],\n",
    "                    annot_str= names[np.ix_(odd_rows,odd_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: calculate normalization volumes and add to plate map\n",
    "\n",
    "This step will calculate volumes for the DNA normalization pick list.\n",
    "\n",
    "Check the desired values for:\n",
    " - **`ng`**: the desired quantity of DNA in normed plate, in ng\n",
    " - **`total_vol`**: the total volume of normalized DNA, in nL\n",
    " - **`min_vol`**: the minimum quantity of sample to add, in nL\n",
    " - **`resolution`**: the resolution of the Echo, in nL (usually 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = 5\n",
    "total_vol = 3500\n",
    "min_vol = 25\n",
    "resolution = 2.5\n",
    "\n",
    "dna_vols = calculate_norm_vol(plate_df['Sample DNA Concentration'], ng=ng, min_vol=min_vol, max_vol=total_vol, resolution=resolution)\n",
    "water_vols = total_vol - dna_vols\n",
    "\n",
    "plate_df['Normalized DNA volume'] = dna_vols\n",
    "plate_df['Normalized water volume'] = water_vols\n",
    "\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a (optional): Add synDNA spike-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_df = add_syndna(plate_df,syndna_pool_number=None,syndna_concentration=2.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plate_df['syndna_pool_number'].unique() != None:\n",
    "    syndna_well='A1'\n",
    "    syndna_plate = 'synDNA plate'\n",
    "    syndna_picklist = format_dna_norm_picklist(np.array(plate_df['synDNA volume']),\n",
    "                                             np.zeros(plate_df.shape[0]),\n",
    "                                             np.repeat(syndna_well,plate_df.shape[0]),\n",
    "                                             dest_wells = np.array(plate_df[well_col]),\n",
    "                                             sample_names = np.array(plate_df['Sample']),\n",
    "                                             sample_plates = np.repeat(syndna_plate,plate_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plate_df['syndna_pool_number'].unique() != None:\n",
    "    # Write the picklist as .txt\n",
    "    syndna_picklist_fp = './test_output/Input_Norm/YYYY_MM_DD_Celeste_Adaptation_16-21_syndna.txt'\n",
    "\n",
    "    if os.path.isfile(syndna_picklist_fp):\n",
    "        print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plate_df['syndna_pool_number'].unique() != None:\n",
    "    with open(syndna_picklist_fp, 'w') as f:\n",
    "        f.write(syndna_picklist)\n",
    "\n",
    "    !head {syndna_picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: make pick list\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_picklist = format_dna_norm_picklist(np.array(plate_df['Normalized DNA volume']),\n",
    "                                         np.array(plate_df['Normalized water volume']),\n",
    "                                         np.array(plate_df['Well']),\n",
    "                                         dest_wells = np.array(plate_df[well_col]),\n",
    "                                         sample_names = np.array(plate_df['Sample']),\n",
    "                                         sample_plates = np.array(plate_df['Compressed Plate Name']),\n",
    "                                         dna_concs = np.array(plate_df['Sample DNA Concentration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: write pick list to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the picklist as .txt\n",
    "norm_picklist_fp = './test_output/Input_Norm/YYYY_MM_DD_Celeste_Adaptation_16-21_inputnorm.txt'\n",
    "\n",
    "if os.path.isfile(norm_picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(norm_picklist_fp, 'w') as f:\n",
    "    f.write(norm_picklist)\n",
    "    \n",
    "!head {norm_picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for assigning barcodes\n",
    "\n",
    "This portion of the notebook will assign index values and construct an Echo picklist file for adding barcodes. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A plate map dataframe (from previous step)\n",
    "2. A tab-delimited index combination file, relating index combinations, i5 and i7 index values, and i5 and i7 index locations\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the index combo list\n",
    "2. assigns indices per sample\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in index combo list\n",
    "\n",
    "This is a file that contains every possible i5 and i7 barcode combo on a separate line,\n",
    "along with plate and well location information. It should look something like this:\n",
    "\n",
    "```\n",
    "index combo,index combo seq,i5 name,i5 sequence,i5 well,i5 plate,i7 name,i7 sequence,i7 well,i7 plate\n",
    "0,ACCGACAAACGTTACC,iTru5_01_A,ACCGACAA,A1,iTru5_plate,iTru7_101_01,ACGTTACC,A1,iTru7_plate\n",
    "1,AGTGGCAACTGTGTTG,iTru5_01_B,AGTGGCAA,B1,iTru5_plate,iTru7_101_02,CTGTGTTG,A2,iTru7_plate\n",
    "2,CACAGACTTGAGGTGT,iTru5_01_C,CACAGACT,C1,iTru5_plate,iTru7_101_03,TGAGGTGT,A3,iTru7_plate\n",
    "3,CGACACTTGATCCATG,iTru5_01_D,CGACACTT,D1,iTru5_plate,iTru7_101_04,GATCCATG,A4,iTru7_plate\n",
    "4,GACTTGTGGCCTATCA,iTru5_01_E,GACTTGTG,E1,iTru5_plate,iTru7_101_05,GCCTATCA,A5,iTru7_plate\n",
    "5,GTGAGACTAACAACCG,iTru5_01_F,GTGAGACT,F1,iTru5_plate,iTru7_101_06,AACAACCG,A6,iTru7_plate\n",
    "6,GTTCCATGACTCGTTG,iTru5_01_G,GTTCCATG,G1,iTru5_plate,iTru7_101_07,ACTCGTTG,A7,iTru7_plate\n",
    "7,TAGCTGAGCCTATGGT,iTru5_01_H,TAGCTGAG,H1,iTru5_plate,iTru7_101_08,CCTATGGT,A8,iTru7_plate\n",
    "8,CTTCGCAATGTACACC,iTru5_02_A,CTTCGCAA,I1,iTru5_plate,iTru7_101_09,TGTACACC,A9,iTru7_plate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_combo_fp = './test_output/iTru/new_iTru_combos_Dec2017.csv'\n",
    "\n",
    "if not os.path.isfile(index_combo_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_combos = pd.read_csv(index_combo_fp)\n",
    "index_combos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Assign index combo\n",
    "\n",
    "This will pick a set of index combos from the index combo for the number of samples in the `plate_df` DataFrame.\n",
    "\n",
    "Keep track of the barcode combinations used in the lab, and set `starting_combo` equal to the next unused combination.\n",
    "\n",
    "One of way of doing that might be to keep track of the number of plates run, and set `starting_combo` equal to\n",
    "384 * number of plates run + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_counter = 144\n",
    "\n",
    "starting_combo = ((plate_counter - 1) % 384) * 384\n",
    "\n",
    "indices = assign_index(len(plate_df['Sample']), index_combos, start_idx=starting_combo).reset_index()\n",
    "\n",
    "plate_df = pd.concat([plate_df, indices], axis=1)\n",
    "\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make index pick list\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_picklist = format_index_picklist(plate_df['Sample'], plate_df[well_col], indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: write pick list to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the picklist as .txt\n",
    "index_picklist_fp = './test_output/Indices/YYYY_MM_DD_Celeste_Adaptation_16-21_indices.txt'\n",
    "\n",
    "if os.path.isfile(index_picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(index_picklist_fp, 'w') as f:\n",
    "    f.write(index_picklist)\n",
    "\n",
    "!head {index_picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for calculating pooling\n",
    "\n",
    "This portion of the notebook calculates pooling based on fluorescente quantification values, and produces visual outputs to interpret and check values. \n",
    "\n",
    "As inputs, this workflow requires:\n",
    "1. A plate map DataFrame (from previous step)\n",
    "2. MiniPico output (tab-delimited text format with columns 'Concentration' and 'Well')\n",
    "\n",
    "The workflow:\n",
    "1. reads in MiniPico output and calculates estimated library concentration\n",
    "4. calculates pooling values and generates an Echo pick list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: read in MiniPico library concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter correct path to MiniPico file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_concs_fp = './test_data/Quant/MiniPico/2022_07_Celeste_Adaptation_16_17_18_21_CleanLib_quant.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_concs = read_pico_csv(lib_concs_fp, plate_reader='SpectraMax_i3x',\n",
    "                          conc_col_name='MiniPico Library DNA Concentration')\n",
    "\n",
    "plate_df = pd.merge(plate_df, lib_concs, on='Well')\n",
    "\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: calculate sample concentration from MiniPico\n",
    "\n",
    "You will want to make sure that 'size' is correct for your average library size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_df['MiniPico Library Concentration'] = compute_pico_concentration(plate_df['MiniPico Library DNA Concentration'],\n",
    "                                                                        size=500)\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: visualization of MiniPico values\n",
    "\n",
    "This step will present visuals of the results, including:\n",
    "1. Scatter plot of DNA concentrations by Library concentration\n",
    "2. Plate-wise heatmap and histogram showing library concentrations\n",
    "3. per-96-well plate heatmaps and histograms showing library concentrations and sample names\n",
    "4. Plate-wise heatmap showing pooling values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library concentration by sample DNA concentration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "plate_df['Input DNA'] = plate_df['Sample DNA Concentration']*plate_df['Normalized DNA volume']/1000\n",
    "sns.regplot(x=\"Sample DNA Concentration\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax1);\n",
    "sns.boxplot(x=\"Blank\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax2);\n",
    "sns.swarmplot(x=\"Blank\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax2,\n",
    "              size=3,color='black',alpha=0.5)\n",
    "sns.scatterplot( x=\"Input DNA\",y=\"MiniPico Library DNA Concentration\",hue='Sample DNA Concentration',data=plate_df ,ax = ax3);\n",
    "ax3.legend(title='Sample DNA Concentration',loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanks_gdna_concs = plate_df.loc[plate_df['Blank']==True,'Sample DNA Concentration']\n",
    "samples_gdna_concs = plate_df.loc[plate_df['Blank']==False,'Sample DNA Concentration']\n",
    "mannwhitneyu(samples_gdna_concs, blanks_gdna_concs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanks_lib_concs = plate_df.loc[plate_df['Blank']==True,'MiniPico Library Concentration']\n",
    "samples_lib_concs = plate_df.loc[plate_df['Blank']==False,'MiniPico Library Concentration']\n",
    "mannwhitneyu(samples_lib_concs, blanks_lib_concs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library concentration heatmap, whole plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get concentration and pooling values for plotting\n",
    "concs = make_2D_array(plate_df, data_col=\"MiniPico Library Concentration\", well_col=well_col).astype(float)\n",
    "dna = make_2D_array(plate_df, data_col='Sample DNA Concentration', well_col=well_col).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "names = make_2D_array(plate_df, data_col='Sample', well_col=well_col)\n",
    "i5 = make_2D_array(plate_df, data_col='i5 name', well_col=well_col)\n",
    "i7 = make_2D_array(plate_df, data_col='i7 name', well_col=well_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(concs, color_map='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate maps for individual constituent plates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library concentration heatmap, Plate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(concs[np.ix_(even_rows,even_cols)],\n",
    "                    annot_str= names[np.ix_(even_rows,even_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library concentration heatmap, Plate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(concs[np.ix_(even_rows,odd_cols)],\n",
    "                    annot_str= names[np.ix_(even_rows,odd_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library concentration heatmap, Plate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(concs[np.ix_(odd_rows,even_cols)],\n",
    "                    annot_str= names[np.ix_(odd_rows,even_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library concentration heatmap, Plate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plate_vals(concs[np.ix_(odd_rows,odd_cols)],\n",
    "                    annot_str= names[np.ix_(odd_rows,odd_cols)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: calculate pooling values for MiniPico with autopool\n",
    "\n",
    "This step will calculate the sample pooling, and update the sample data frame with the pool info.\n",
    "There are two automated methods to pool:\n",
    "1. **norm**: This will attempt to generate a normalized pool, automatically inferring the best parameter for pooling.\n",
    "    - ***pool_failures***:\n",
    "        - _high_: will pool failures at the highest pooling volume from optimized pooling.\n",
    "        - _low_: will pool failures at the lowest pooling volume from optimized pooling.\n",
    "\n",
    "2. **evp**: This will pool an even volume per sample.\n",
    "    - ***total_vol***: (Optional, Default: 100ÂµL) The total volume to pool, in uL. Each sample will be pooled at 1/N of that volume, where N is total number of samples in the prep.\n",
    "\n",
    "3. **automate**: (Optional, Default = True) When False, this argument will allow one input parameters for **Legacy** arguments. \n",
    "\n",
    "> **Legacy**\n",
    "> There are legacy parameters to control pooling behaviors when autopool automation (automate=True) returns a poor result. To use these parameters, one must pass automate=False.\n",
    "\n",
    ">   - **min_conc**: (default: 0) This is the minimum concentration for a sample to be considered for pooling.\n",
    "    Set to 0 to pool all samples, regardless of concentration. Increasing this will have the \n",
    "    effect of increasing pool concentration, at the expense of samples dropping out. \n",
    ">   - **floor_conc**: This is the lowest concentration equivalent for which a sample will be \n",
    "    accurately pooled. Samples below this concentration will be pooled to the volume that they \n",
    "    would have been if they were actually that concentration. For example, if `floor_conc=20`, \n",
    "    and a sample at 20 nM pools at 500 nL, a sample at 40 nM will pool at 250 nL but a sample at \n",
    "    10 nM will still pool at 500 nL (rather than 1000). Increasing this value will have the effect \n",
    "    of increasing pool concentration, but decreasing read counts for low-concentration samples. \n",
    ">   - **total_nmol**: This is the total number of molecules to shoot for in the pool. Increasing\n",
    "    this will increase the overall volume of the pool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot pooling volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = find_threshold(plate_df['MiniPico Library Concentration'], plate_df['Blank'])\n",
    "plate_df = autopool(plate_df,method='evp',total_vol=190)\n",
    "\n",
    "# visualize\n",
    "print(\"Floor concentration: {}\".format(threshold))\n",
    "vols = make_2D_array(plate_df, data_col='MiniPico Pooled Volume', well_col=well_col).astype(float)\n",
    "conc, vol = estimate_pool_conc_vol(plate_df['MiniPico Pooled Volume'], plate_df['MiniPico Library Concentration'])\n",
    "print(\"Pool concentration: {:.2f}\".format(conc))\n",
    "print(\"Pool volume: {:.2f}\".format(vol))\n",
    "with suppress(np.linalg.LinAlgError):\n",
    "    plot_plate_vals(vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vols = make_2D_array(plate_df, data_col='MiniPico Pooled Volume', well_col=well_col).astype(float)\n",
    "sns.scatterplot(x='MiniPico Library Concentration', y='MiniPico Pooled Volume',data=plate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: write pooling pick list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the picklist as .csv\n",
    "picklist_fp = './test_output/Pooling/YYYY_MM_DD_Celeste_Adaptation_evp.csv'\n",
    "\n",
    "if os.path.isfile(picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklist = format_pooling_echo_pick_list(vols, max_vol_per_well=30000)\n",
    "# A max_vol_per_well of 30,000 nL is used to prevent overfilling the destination well, which is upside down inside Echo instrument.\n",
    "# Overfilling the destination well could lead to contamination of source plate as gravity forces on water in destination plate \n",
    "# might be stronger than superficial tension that is holding the pool inside the destination well, which is upside down.\n",
    "with open(picklist_fp,'w') as f:\n",
    "    f.write(picklist)\n",
    "\n",
    "!head {picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write DataFrame to file\n",
    "\n",
    "We want to keep all that useful information together in one place so it can be easily parsed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sample info DataFrame as .txt\n",
    "plate_df_fp = './test_output/QC/YYYY_MM_DD_Celeste_Adaptation_df.txt'\n",
    "\n",
    "if os.path.isfile(plate_df_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_df.to_csv(plate_df_fp, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Make sample sheet\n",
    "\n",
    "This workflow takes the pooled sample information and writes an Illumina sample sheet that can be given directly to the sequencing center. \n",
    "\n",
    "As inputs, this notebook requires:\n",
    "1. A plate map DataFrame (from previous step)\n",
    "\n",
    "The workflow:\n",
    "1. formats sample names as bcl2fastq-compatible\n",
    "2. formats sample data\n",
    "3. sets values for sample sheet fields and formats sample sheet\n",
    "4. writes the sample sheet to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Format sample names to be bcl2fastq-compatible\n",
    "\n",
    "bcl2fastq allows *only* alphanumeric, hyphens, and underscore characters. We'll replace all non-those characters\n",
    "with underscores and add the bcl2fastq-compatible names to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_df['sample sheet Sample_ID'] = plate_df['Sample'].map(bcl_scrub_name)\n",
    "\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: format sample sheet data\n",
    "\n",
    "This step formats the data columns appropriately for the sample sheet, using the values we've calculated previously.\n",
    "\n",
    "The newly-created bcl2fastq-compatible names will be in the **`Sample ID`** and **`Sample Name`** columns. The\n",
    "original sample names will be in the **`Description`** column.\n",
    "\n",
    "Modify **`lanes`** to indicate which lanes this pool will be sequenced on.\n",
    "\n",
    "**`Project Name`** and **`Project Plate`** values will be placed in the **`Sample_Project`** and **`Sample_Name`**\n",
    "columns, respectively.\n",
    "\n",
    "**`sequencer`** is important for making sure the i5 index is in the correct orientation for demultiplexing. `NovaSeq6000`, `HiSeq4000`, `HiSeq3000`, `NextSeq`, `MiniSeq`, and `iSeq` all require reverse-complemented i5 index sequences. If you enter one of these exact strings in for `sequencer`, it will revcomp the i5 sequence for you.\n",
    "\n",
    "`HiSeq2500`, `MiSeq`, `NovaSeqX`, and `NovaSeqXPlus` will not revcomp the i5 sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = 'iSeq'\n",
    "lanes = [1]\n",
    "plate_df['vol_extracted_elution_ul'] = 70\n",
    "\n",
    "# Knight Lab Nextera is also valid for library_construction_protocol\n",
    "metadata = {\n",
    "    'Bioinformatics': [\n",
    "        {\n",
    "         'Sample_Project': 'Celeste_Adaptation_12986',\n",
    "         'QiitaID': '12986',\n",
    "         'BarcodesAreRC': 'True',\n",
    "         'ForwardAdapter': 'GATCGGAAGAGCACACGTCTGAACTCCAGTCAC',\n",
    "         'ReverseAdapter': 'GATCGGAAGAGCGTCGTGTAGGGAAAGGAGTGT',\n",
    "         'HumanFiltering': 'True',\n",
    "         'library_construction_protocol': 'Knight Lab Kapa HyperPlus',\n",
    "         'experiment_design_description': 'isolate sequencing',\n",
    "         'contains_replicates':plate_df['contains_replicates'].all(),\n",
    "        },\n",
    "    ],\n",
    "    'Contact': [\n",
    "        {\n",
    "         'Sample_Project': 'Celeste_Adaptation_12986',\n",
    "         # non-admin contacts who want to know when the sequences\n",
    "         # are available in Qiita\n",
    "         'Email': 'rodolfo.salido@gmail.com'\n",
    "        },\n",
    "    ],\n",
    "    'Assay': 'Metagenomic',\n",
    "    'SheetType': 'standard_metag',\n",
    "    'SheetVersion':'100'\n",
    "}\n",
    "\n",
    "sheet = make_sample_sheet(metadata, plate_df, sequencer, lanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write the sample sheet to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the samplesheet as .csv\n",
    "sample_sheet_fp = './test_output/SampleSheets/YYYY_MM_DD_Celeste_Adaptation_12986_16_17_18_21_samplesheet.csv'\n",
    "\n",
    "if os.path.isfile(sample_sheet_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_sheet_fp,'w') as f:\n",
    "    sheet.write(f)\n",
    "    \n",
    "!head -n 30 {sample_sheet_fp}\n",
    "!echo ...\n",
    "!tail -n 15 {sample_sheet_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Read Distribution Summary and Pool Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: import and merge per_sample read distributions\n",
    "\n",
    "Import a tsv file with read_counts from per_sample_fastq files and merge with growing plate_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reads counts from file to dataframes\n",
    "\n",
    "read_counts_df = pd.read_csv('./test_data/Demux/YYYY_MM_DD_Celeste_Adaptation_16_17_18_21_raw_counts.tsv',\n",
    "                                 sep='\\t')\n",
    "raw_read_counts_df = read_counts_df.loc[~read_counts_df['Category'].str.contains('trimmed')]\n",
    "filtered_read_counts_df = read_counts_df.loc[read_counts_df['Category'].str.contains('trimmed')]\n",
    "\n",
    "##Can also import counts from Qiita per_sample_FASTQ summaries.  \n",
    "# per_sample_fastq_counts_df = pd.read_csv('./test_data/Demux/YYYY_MM_DD_Celeste_Adaptation_16_17_18_21_per_sample_fastq.tsv',\n",
    "#                                          sep='\\t')\n",
    "\n",
    "#Can also import reads counts from qiita prep-file file to dataframe\n",
    "# read_counts_df = pd.read_csv('./test_data/Demux/15288_prep_16673_20240401-173251.txt',\n",
    "#                                  sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge read_counts_df with plate_df \n",
    "\n",
    "plate_df_w_reads = merge_read_counts(plate_df,raw_read_counts_df, reads_column_name='Raw Reads')\n",
    "\n",
    "plate_df_w_reads = merge_read_counts(plate_df_w_reads,filtered_read_counts_df,\n",
    "                                     reads_column_name='Filtered Reads')\n",
    "\n",
    "# plate_df_w_reads = merge_read_counts(plate_df_w_reads,per_sample_fastq_counts_fp,\n",
    "#                                      reads_column_name='Qiita Reads')\n",
    "\n",
    "plate_df_w_reads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_column = 'Raw Reads'\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "# evenness plot\n",
    "rmax = int(round(plate_df_w_reads[reads_column].max(),-2))\n",
    "survival_df = pd.concat([read_survival(plate_df_w_reads.loc[plate_df_w_reads['Blank'] == True,\n",
    "                                                            reads_column], label='Blanks',rmax=rmax),\n",
    "                         read_survival(plate_df_w_reads.loc[plate_df_w_reads['Blank'] == False,\n",
    "                                                            reads_column], label='Samples',rmax=rmax)])\n",
    "\n",
    "ax3.set_xlabel(reads_column)\n",
    "ax3.set_ylabel('Samples')\n",
    "survival_df.plot(color = ['coral','steelblue'],ax=ax1)\n",
    "ax1.set_xlabel(reads_column)\n",
    "ax1.set_ylabel('Samples')\n",
    "\n",
    "##Histogram\n",
    "sns.histplot(plate_df_w_reads[reads_column],ax=ax3)\n",
    "\n",
    "##Regressopm\n",
    "sns.regplot(x=\"MiniPico Library DNA Concentration\", y=reads_column, data=plate_df_w_reads, ax = ax2);\n",
    "\n",
    "#Boxplot\n",
    "sns.boxplot(x=\"Blank\", y=reads_column, data=plate_df_w_reads, ax = ax4);\n",
    "sns.stripplot(x=\"Blank\", y=reads_column, data=plate_df_w_reads, ax = ax4,\n",
    "              size=3,color='black',alpha=0.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate iSeqnorm pooling volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_df_normalized = calculate_iseqnorm_pooling_volumes(plate_df_w_reads,dynamic_range=5, normalization_column='Raw Reads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "vols = make_2D_array(plate_df_normalized, data_col='iSeq normpool volume', well_col=well_col).astype(float)\n",
    "conc, vol = estimate_pool_conc_vol(plate_df_normalized['iSeq normpool volume'], plate_df_normalized['MiniPico Library Concentration'])\n",
    "print(\"Pool concentration: {:.2f}\".format(conc))\n",
    "print(\"Pool volume: {:.2f}\".format(vol))\n",
    "with suppress(np.linalg.LinAlgError):\n",
    "    plot_plate_vals(vols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate read depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots estimate of read depth proportion, and returns a df with estimates. \n",
    "plate_df_normalized_with_estimates = estimate_read_depth(plate_df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: write pooling picklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the picklist as .csv\n",
    "picklist_fp = './test_output/Pooling/YYYY_MM_DD_Celeste_Adaptation_16_17_18_21_iSeqnormpool.csv'\n",
    "\n",
    "if os.path.isfile(picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklist = format_pooling_echo_pick_list(vols, max_vol_per_well=30000)\n",
    "with open(picklist_fp,'w') as f:\n",
    "    f.write(picklist)\n",
    "\n",
    "!head {picklist_fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "473px",
    "width": "381px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "740px",
    "left": "0px",
    "right": "1407.6666259765625px",
    "top": "112px",
    "width": "279px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
