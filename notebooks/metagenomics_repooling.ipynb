{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Outline\" data-toc-modified-id=\"Outline-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Outline</a></span><ul class=\"toc-item\"><li><span><a href=\"#steps:\" data-toc-modified-id=\"steps:-0.1.1\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>steps:</a></span></li><li><span><a href=\"#Step-1:-Read-in-output-pandas-summary-DFs-from-each-pool\" data-toc-modified-id=\"Step-1:-Read-in-output-pandas-summary-DFs-from-each-pool-0.1.2\"><span class=\"toc-item-num\">0.1.2&nbsp;&nbsp;</span>Step 1: Read in output pandas summary DFs from each pool</a></span></li><li><span><a href=\"#Fix-some-gaps-in-the-data\" data-toc-modified-id=\"Fix-some-gaps-in-the-data-0.1.3\"><span class=\"toc-item-num\">0.1.3&nbsp;&nbsp;</span>Fix some gaps in the data</a></span></li><li><span><a href=\"#Step-3:-read-in-combined-read-counts\" data-toc-modified-id=\"Step-3:-read-in-combined-read-counts-0.1.4\"><span class=\"toc-item-num\">0.1.4&nbsp;&nbsp;</span>Step 3: read in combined read counts</a></span></li><li><span><a href=\"#Step-4:-calculate-per-nL-reads-value\" data-toc-modified-id=\"Step-4:-calculate-per-nL-reads-value-0.1.5\"><span class=\"toc-item-num\">0.1.5&nbsp;&nbsp;</span>Step 4: calculate per-nL reads value</a></span></li><li><span><a href=\"#Step-5:-calculate-additional-nL-necessary-to-achieve-target\" data-toc-modified-id=\"Step-5:-calculate-additional-nL-necessary-to-achieve-target-0.1.6\"><span class=\"toc-item-num\">0.1.6&nbsp;&nbsp;</span>Step 5: calculate additional nL necessary to achieve target</a></span></li><li><span><a href=\"#Step-6:-rank-list-based-on-addn'l-nL-needed\" data-toc-modified-id=\"Step-6:-rank-list-based-on-addn'l-nL-needed-0.1.7\"><span class=\"toc-item-num\">0.1.7&nbsp;&nbsp;</span>Step 6: rank list based on addn'l nL needed</a></span></li><li><span><a href=\"#Step-7:-find-/-visualize-cutoffs-for-???\" data-toc-modified-id=\"Step-7:-find-/-visualize-cutoffs-for-???-0.1.8\"><span class=\"toc-item-num\">0.1.8&nbsp;&nbsp;</span>Step 7: find / visualize cutoffs for ???</a></span></li><li><span><a href=\"#Step-8:-evaluate-samples-that-need-to-be-RE-LIBRARY'd\" data-toc-modified-id=\"Step-8:-evaluate-samples-that-need-to-be-RE-LIBRARY'd-0.1.9\"><span class=\"toc-item-num\">0.1.9&nbsp;&nbsp;</span>Step 8: evaluate samples that need to be RE-LIBRARY'd</a></span></li></ul></li><li><span><a href=\"#Distribution-of-remaining-sequences\" data-toc-modified-id=\"Distribution-of-remaining-sequences-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Distribution of remaining sequences</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries-to-be-remade-entirely\" data-toc-modified-id=\"Libraries-to-be-remade-entirely-0.2.1\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>Libraries to be remade entirely</a></span></li></ul></li><li><span><a href=\"#Plot:-failures-over-time,-defined-by-different-seqs-per-nL-thresholds\" data-toc-modified-id=\"Plot:-failures-over-time,-defined-by-different-seqs-per-nL-thresholds-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Plot: failures over time, defined by different seqs per nL thresholds</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries-fit-to-repool\" data-toc-modified-id=\"Libraries-fit-to-repool-0.3.1\"><span class=\"toc-item-num\">0.3.1&nbsp;&nbsp;</span>Libraries fit to repool</a></span></li><li><span><a href=\"#Plate-5-8-libraries\" data-toc-modified-id=\"Plate-5-8-libraries-0.3.2\"><span class=\"toc-item-num\">0.3.2&nbsp;&nbsp;</span>Plate 5-8 libraries</a></span></li></ul></li><li><span><a href=\"#Barcode-figuring-out\" data-toc-modified-id=\"Barcode-figuring-out-0.4\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>Barcode figuring out</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-9:-evaluate-lane-splits\" data-toc-modified-id=\"Step-9:-evaluate-lane-splits-0.4.1\"><span class=\"toc-item-num\">0.4.1&nbsp;&nbsp;</span>Step 9: evaluate lane splits</a></span></li><li><span><a href=\"#Step-10:-create-new-barcode-file-for-remade-plates-5-8-and-new-samples\" data-toc-modified-id=\"Step-10:-create-new-barcode-file-for-remade-plates-5-8-and-new-samples-0.4.2\"><span class=\"toc-item-num\">0.4.2&nbsp;&nbsp;</span>Step 10: create new barcode file for remade plates 5-8 and new samples</a></span></li><li><span><a href=\"#Step-11:-make-repool-pick-list-for-set-1-samples\" data-toc-modified-id=\"Step-11:-make-repool-pick-list-for-set-1-samples-0.4.3\"><span class=\"toc-item-num\">0.4.3&nbsp;&nbsp;</span>Step 11: make repool pick list for set 1 samples</a></span></li><li><span><a href=\"#Step-11:-make-repool-pick-list-for-set-2-samples\" data-toc-modified-id=\"Step-11:-make-repool-pick-list-for-set-2-samples-0.4.4\"><span class=\"toc-item-num\">0.4.4&nbsp;&nbsp;</span>Step 11: make repool pick list for set 2 samples</a></span></li><li><span><a href=\"#Remake-libraries\" data-toc-modified-id=\"Remake-libraries-0.4.5\"><span class=\"toc-item-num\">0.4.5&nbsp;&nbsp;</span>Remake libraries</a></span></li><li><span><a href=\"#Check-5-8-against-new-quants\" data-toc-modified-id=\"Check-5-8-against-new-quants-0.4.6\"><span class=\"toc-item-num\">0.4.6&nbsp;&nbsp;</span>Check 5-8 against new quants</a></span></li></ul></li><li><span><a href=\"#Exporting-new-data-for-the-combined-plate-5-8-repick-and-plate-81-83\" data-toc-modified-id=\"Exporting-new-data-for-the-combined-plate-5-8-repick-and-plate-81-83-0.5\"><span class=\"toc-item-num\">0.5&nbsp;&nbsp;</span>Exporting new data for the combined plate 5-8 repick and plate 81-83</a></span></li></ul></li><li><span><a href=\"#FINRISK-FOR-JANSSEN-ADENDUM\" data-toc-modified-id=\"FINRISK-FOR-JANSSEN-ADENDUM-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>FINRISK FOR JANSSEN ADENDUM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-read-counts-for-these-samples\" data-toc-modified-id=\"Get-read-counts-for-these-samples-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get read counts for these samples</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from metapool.metapool import *\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%watermark -i -v -iv -m -h -p metapool,sample_sheet,openpyxl -u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "### steps:\n",
    "1. read in output pandas summary DFs from each constituent pool\n",
    "  - add a pool / plate column\n",
    "  - combine\n",
    "2. read in output plate survey\n",
    "  - add to combined DF per-plate\n",
    "3. read in combined read counts\n",
    "  - merge with DF on sample name (may need to munge sample names)\n",
    "4. calculate per-nL reads value\n",
    "5. calculate additional nL necessary to achieve target\n",
    "6. rank list based on addn'l nL needed\n",
    "7. find / visualize cutoffs for ???\n",
    "8. evaluate samples that need to be RE-LIBRARY'd\n",
    "  - create DNA pick-list, sep by source plate\n",
    "  - can input into pooling run\n",
    "9. evaluate samples that can be TOPPED-OFF\n",
    "  - samples not being re-librarie'd that can be successfully repooled\n",
    "  - figure out how many total reads --> lane frac --> pooling #s\n",
    "10. estimate combined pool of new libraries\n",
    "  - should include fudge factor --> oversequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in output pandas summary DFs from each pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_plate_dfs(plate_dfs):\n",
    "    lib_df = pd.DataFrame()\n",
    "    \n",
    "    for plate in plate_dfs:\n",
    "        p = pd.read_csv(plate_dfs[plate], sep='\\t', index_col=0)\n",
    "        p['Pool name'] = plate\n",
    "        p['Pool fraction'] = p['Pooled Volume'] / p['Pooled Volume'].sum()\n",
    "        lib_df = pd.concat([lib_df, p])\n",
    "        print(p['Pooled Volume'].sum())\n",
    "    lib_df = lib_df.rename_axis('Pool Index').reset_index()\n",
    "        \n",
    "    return(lib_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./finrisk_data/complete_dataframes/FinRisk_5-8_back-compiled_dummy.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of plates\n",
    "\n",
    "plate_dfs = {\n",
    "             'Finrisk_01-04': './finrisk_data/complete_dataframes/FinRisk_1-4_back-compiled.txt',\n",
    "             'Finrisk_05-08': './finrisk_data/complete_dataframes/FinRisk_5-8_back-compiled_dummy.txt',\n",
    "             'Finrisk_09-12': './finrisk_data/complete_dataframes/FinRisk_9-12_back-compiled.txt',\n",
    "             'Finrisk_13-16': './finrisk_data/complete_dataframes/Finrisk_13-16_dataframe.txt',\n",
    "             'Finrisk_17-20': './finrisk_data/complete_dataframes/FinRisk_17-20_data_frame.txt',\n",
    "             'Finrisk_21-24': './finrisk_data/complete_dataframes/FinRisk_21-24_Data_Frame.txt',\n",
    "             'Finrisk_25-28': './finrisk_data/complete_dataframes/Finrisk_25-28_dataframe.txt',\n",
    "             'Finrisk_29-32': './finrisk_data/complete_dataframes/FinRisk_29-32_dataframe.txt',\n",
    "             'Finrisk_33-36': './finrisk_data/complete_dataframes/FinRisk_33-36_data_frame.txt',\n",
    "             'Finrisk_37-40': './finrisk_data/complete_dataframes/FinRisk_37-40_plate_dataframe.txt',\n",
    "             'Finrisk_41-44': './finrisk_data/complete_dataframes/FinRisk_41-44_data_frame.txt',\n",
    "             'Finrisk_45-48': './finrisk_data/complete_dataframes/FinRisk_45-48_plate_dataframe.txt',\n",
    "             'Finrisk_49-52': './finrisk_data/complete_dataframes/FinRisk_49-52_dataframe.txt',\n",
    "             'Finrisk_53-56': './finrisk_data/complete_dataframes/Finrisk_53-56_dataframe.txt',\n",
    "             'Finrisk_57-60': './finrisk_data/complete_dataframes/FinRisk_57-60_plate_dataframe.txt',\n",
    "             'Finrisk_61-64': './finrisk_data/complete_dataframes/FinRIsk_61-64_DataFrame.txt',\n",
    "             'Finrisk_65-68': './finrisk_data/complete_dataframes/FinRisk_65-68_dataframe.txt',\n",
    "             'Finrisk_69-72': './finrisk_data/complete_dataframes/Finrisk_69-72_dataframe.txt',\n",
    "             'Finrisk_73-76': './finrisk_data/complete_dataframes/FinRisk_73-76_data_frame.txt',\n",
    "             'Finrisk_77-80': './finrisk_data/complete_dataframes/FinRisk_77-80_DataFrame.txt'}\n",
    "\n",
    "# read in plates\n",
    "lib_df = read_plate_dfs(plate_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lib_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix some gaps in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plate 13-16 didn't have Library Well value; but these are not reorganized, so 'Well' is fine\n",
    "\n",
    "lib_df.loc[lib_df['Pool name'] == 'Finrisk_13-16','Library Well'] = lib_df.loc[lib_df['Pool name'] == 'Finrisk_13-16','Well'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df.loc[lib_df['Pool name'] == 'Finrisk_13-16',['Pool name','Sample','Well','Library Well']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plate 17-20 is missing info from norm pick list\n",
    "\n",
    "lib_df.loc[lib_df['Pool name'] == 'Finrisk_17-20',['Pool name','Sample', 'Well','Library Well','Sample DNA Concentration','Normalized DNA volume','Normalized water volume']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import from pick list and add to DF\n",
    "\n",
    "finrisk_17_20_norm = pd.read_csv('./finrisk_data/Input_Norm/Fin Risk 17-20 Input Norm - Echo Picklist.csv')\n",
    "finrisk_17_20_norm.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update values\n",
    "\n",
    "for i, row in finrisk_17_20_norm.loc[finrisk_17_20_norm['Source Plate Name'] == 'water',].iterrows():\n",
    "    well = row['Source Well']\n",
    "    vol = row['Transfer Volume']\n",
    "    \n",
    "    lib_df.loc[(lib_df['Pool name'] == 'Finrisk_17-20') & (lib_df['Well'] == well), 'Normalized water volume'] = vol\n",
    "\n",
    "for i, row in finrisk_17_20_norm.loc[finrisk_17_20_norm['Source Plate Name'] != 'water',].iterrows():\n",
    "    well = row['Source Well']\n",
    "    vol = row['Transfer Volume']\n",
    "    \n",
    "    lib_df.loc[(lib_df['Pool name'] == 'Finrisk_17-20') & (lib_df['Well'] == well), 'Normalized DNA volume'] = vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df.loc[lib_df['Pool name'] == 'Finrisk_17-20',['Pool name','Sample', 'Well','Library Well','Sample DNA Concentration','Normalized DNA volume','Normalized water volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df.to_csv('./finrisk_data/Re-pooling/complete_lib_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df.to_csv('lib_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: read in combined read counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiqc_out(fp, regex='^(.+?)\\.(R\\d)\\.trimmed\\.filtered\\..+$'):\n",
    "    reads_df = pd.read_csv(fp, sep='\\t', header=0)\n",
    "    \n",
    "    # extract read and sample name\n",
    "    reads_df[['Sample','Read']] = reads_df['Sample'].str.extract(regex, expand=True)\n",
    "    \n",
    "    return(reads_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in combined read counts\n",
    "\n",
    "reads_fp = './finrisk_data/multiqc_fastqc.txt'\n",
    "\n",
    "reads_df = read_multiqc_out(reads_fp)\n",
    "\n",
    "reads_df['log10 Total Sequences'] = np.log10(reads_df['Total Sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reads_df.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reads_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "axs = reads_df.hist(column = 'log10 Total Sequences', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = reads_df.hist(column = 'Total Sequences', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_libs.plot.scatter(x = 'Seqs below target', y = 'nL to target', logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge with other sample info\n",
    "\n",
    "total_df = pd.merge(left=lib_df, right=reads_df.loc[reads_df['Read'] == 'R1',], left_on='Sample', right_on='Sample', how='outer')\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set blank info because it wasn't set in main!\n",
    "\n",
    "total_df['Blank'] = True\n",
    "total_df.loc[total_df['Sample'].str.match('\\d\\d\\d\\d\\d\\d\\d\\d\\d+-\\d'), 'Blank'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: calculate per-nL reads value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some samples were not pooled. We will set to 100 nL for effective calculations (these will be 'failed' libraries)\n",
    "total_df['Effective Pool Volume'] = total_df['Pooled Volume']\n",
    "total_df.loc[total_df['Pooled Volume'] == 0, 'Effective Pool Volume'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['Sequences per nL'] = total_df['Total Sequences'] / total_df['Effective Pool Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = total_df.plot.scatter(x = 'Library Concentration', y = 'Sequences per nL', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = total_df.plot.scatter(x = 'Library Concentration', y = 'Sequences per nL', alpha=0.4, logy=True, logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.sort_values('Pool name', inplace=True)\n",
    "\n",
    "\n",
    "plates = sorted(list(set(total_df['Pool name'].dropna())))\n",
    "colors = sns.color_palette(\"spectral\", len(plates))\n",
    "plate_colors = dict(zip(plates, colors))\n",
    "sns.palplot(colors)\n",
    "col = total_df['Pool name'].map(plate_colors)\n",
    "\n",
    "\n",
    "ax = total_df.plot.scatter(x = 'Library Concentration', y = 'Sequences per nL', c=col, alpha=0.3, logy=True, logx=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['log10 Sequences per nL'] = np.log10(total_df['Sequences per nL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_df.loc[total_df['Pool name'] != 'Finrisk_05-08'].groupby(['Pool name'])['log10 Sequences per nL'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_df.loc[total_df['Pool name'] != 'Finrisk_05-08'].groupby(['Pool name'])['log10 Sequences per nL'].plot.kde();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['log10 Library Concentration'] = np.log10(total_df['Library Concentration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[total_df['Pool name'] != 'Finrisk_05-08'].groupby(['Pool name'])['Library Concentration'].plot.kde();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = ['Finrisk_01-04',\n",
    "        'Finrisk_09-12',\n",
    "        'Finrisk_13-16',\n",
    "        'Finrisk_17-20',\n",
    "        'Finrisk_21-24',\n",
    "        'Finrisk_25-28']\n",
    "\n",
    "set2 = ['Finrisk_29-32',\n",
    "        'Finrisk_33-36',\n",
    "        'Finrisk_37-40',\n",
    "        'Finrisk_41-44',\n",
    "        'Finrisk_45-48',\n",
    "        'Finrisk_49-52']\n",
    "\n",
    "set3 = ['Finrisk_53-56',\n",
    "        'Finrisk_57-60',\n",
    "        'Finrisk_61-64',\n",
    "        'Finrisk_65-68',\n",
    "        'Finrisk_69-72',\n",
    "        'Finrisk_73-76',\n",
    "        'Finrisk_77-80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[total_df['Pool name'].isin(set1),].groupby(['Pool name'])['Library Concentration'].plot.kde();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[total_df['Pool name'].isin(set2),].groupby(['Pool name'])['Library Concentration'].plot.kde();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[total_df['Pool name'].isin(set3),].groupby(['Pool name'])['Library Concentration'].plot.kde();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: calculate additional nL necessary to achieve target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set target number of sequences\n",
    "\n",
    "target = 400000\n",
    "\n",
    "# Create column of differences from target\n",
    "total_df['Seqs below target'] = target - total_df['Total Sequences']\n",
    "\n",
    "# Blanks don't have a target, set them to NaN\n",
    "total_df.loc[total_df['Blank'], 'Seqs below target'] = np.nan\n",
    "\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: rank list based on addn'l nL needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Figuring you have X seqs per nL on previous run, divide target # seqs by seqs per nL to get requisite # of nLs\n",
    "total_df['nL to target'] = total_df['Seqs below target'] / total_df['Sequences per nL']\n",
    "\n",
    "# create a new sub-df containing the list of samples that need sequences\n",
    "low_libs = total_df.loc[total_df['Seqs below target'] > 0,].sort_values('Pool name')\n",
    "low_libs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: find / visualize cutoffs for ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the nL to target against to the # seqs remaining. \n",
    "\n",
    "low_libs.plot.scatter(x = 'nL to target', y = 'Seqs below target', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the vast majority of samples below target need â‰¤ 100 nL of additional library sequenced (given proportional representation on a complete lane equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Sequences per nL', y = 'Seqs below target', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the tail of samples that had very poor concentrations---failed libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Library Concentration', y = 'Seqs below target', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further demonstrating the tail of failed samples also generally showed low concentration estimates with the qPCR assay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Library Concentration', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = sorted(list(set(low_libs['Pool name'].dropna())))\n",
    "colors = sns.color_palette(\"spectral\", len(plates))\n",
    "plate_colors = dict(zip(plates, colors))\n",
    "sns.palplot(colors)\n",
    "col = low_libs['Pool name'].map(plate_colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', c=col, logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', c='Library Concentration', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_01-04',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_09-12',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[(low_libs['Pool name'] == 'Finrisk_09-12') & (low_libs['Effective Pool Volume'] == 100),].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_13-16',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', c='Library Concentration', logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[(low_libs['Pool name'] == 'Finrisk_13-16') & (low_libs['Sequences per nL'] > 1000),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_17-20',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_21-24',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_25-28',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_29-32',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_73-76',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.loc[low_libs['Pool name'] == 'Finrisk_77-80',].plot.scatter(x = 'Total Sequences', y = 'Sequences per nL', logx=True, logy=True, alpha=0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_libs.plot.scatter(x = 'Sequences per nL', y = 'Seqs below target', c=col, logx=True, logy=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the avg concentrations by plate\n",
    "\n",
    "ax = sns.boxplot(x = \"Pool name\", y = \"Library Concentration\", data=total_df.sort_values('Pool name'))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x = \"Pool name\", y = \"Library Concentration\", data=low_libs.sort_values('Pool name'))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x = \"Pool name\", y = \"Sequences per nL\", data=low_libs.sort_values('Pool name'))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: evaluate samples that need to be RE-LIBRARY'd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can:\n",
    "1. remake 'worst' N libraries; combining failures and non-failures\n",
    "2. remake only libraries below a threshold concentration\n",
    "3. remake only libraries above a threshold estimated pooling volume\n",
    "\n",
    "Add each of these values as columns to the uberframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add column of ranks for libraries, based on estimated sequences per nL\n",
    "\n",
    "low_libs['Sequences per nL rank'] = low_libs['Sequences per nL'].rank()\n",
    "\n",
    "low_libs['nL to target rank'] = low_libs['nL to target'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the nL below target for each sample, according to its 'potency rank'. \n",
    "# the vast majority of libraries need very little extra library to meet desired sequence levels\n",
    "\n",
    "low_libs.plot.scatter(x = 'Sequences per nL rank', y = 'nL to target', logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbr of samples for which amount necessary to re-pool exceeds 500 nL threshold\n",
    "\n",
    "low_libs.loc[low_libs['nL to target'] > 500,].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sequences remaining, according to its 'additional volume needed rank'. \n",
    "# the vast majority of libraries need very little extra library to meet desired sequence levels\n",
    "\n",
    "low_libs.plot.scatter(x = 'nL to target rank', y = 'Seqs below target', logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_libs = low_libs.loc[low_libs['Sequences per nL rank'] <= 380,]\n",
    "repool_libs = low_libs.loc[low_libs['Sequences per nL rank'] > 380,]\n",
    "remake_5_8 = total_df.loc[(total_df['Pool name'] == 'Finrisk_05-08') & (total_df['Seqs below target'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remake_libs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_5_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sequences needed for remake libs\n",
    "\n",
    "remake_libs['Seqs below target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sequences needed for repool libs\n",
    "\n",
    "repool_libs['Seqs below target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_5_8['Seqs below target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expressed as a lane fraction\n",
    "\n",
    "mean_seqs_per_lane = total_df.groupby(['Pool name']).sum()['Total Sequences'].mean()\n",
    "\n",
    "remake_libs['Seqs below target'].sum() / mean_seqs_per_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs['Seqs below target'].sum() / mean_seqs_per_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_5_8['Seqs below target'].sum() / mean_seqs_per_lane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of remaining sequences\n",
    "\n",
    "Unlike a normal plate, not all of the samples for which we need normal sequences made will have the same pooling requirements. \n",
    "\n",
    "Thats ok, because we have a mechanism for calculating pooling based on arbitrary requested proportions. But will be good to see what the distribution of pooling expectations looks like anyway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to be remade entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "remake_libs['Seqs below target'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "remake_libs['Sequences per nL'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = remake_libs.loc[remake_libs['Sequences per nL'] < 1000,].groupby('Pool name').count()['Pool Index']\n",
    "\n",
    "levels = [900, 800, 700, 600, 500, 400, 300, 200, 100]\n",
    "for x in levels:\n",
    "    b = remake_libs.loc[remake_libs['Sequences per nL'] < x,].groupby('Pool name').count()['Pool Index']\n",
    "    a = pd.concat([a,b], axis=1)\n",
    "\n",
    "colnames =  [1000, 900, 800, 700, 600, 500, 400, 300, 200, 100]\n",
    "a.columns = colnames\n",
    "a.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"spectral\", len(colnames))\n",
    "thresh_colors = dict(zip(colnames, colors))\n",
    "thresh_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: failures over time, defined by different seqs per nL thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = a.plot(color=colors, alpha = 0.4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are generally like failed libraries---most need to be pooled at high values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "remake_libs.plot.scatter(x = 'Seqs below target', y = 'nL to target', logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_libs.plot.scatter(x = 'Sequences per nL', y = 'nL to target', logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "400000/ 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries fit to repool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs[['Seqs below target', 'nL to target']].hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are more like top-offs---the bulk of them don't need a whole share, just a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.plot.scatter(x = 'Seqs below target', y = 'nL to target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plate 5-8 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_5_8['Seqs below target'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are generally failed libraries as well---might as well repool at targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barcode figuring out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(low_libs['index combo seq'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barcode collisions among complete low-lib set\n",
    "\n",
    "len(repool_libs['index combo seq'].dropna()) - len(set(repool_libs['index combo seq'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count max number of uses of a barcode combo\n",
    "\n",
    "combo_uses = Counter(low_libs['index combo seq'])\n",
    "\n",
    "sorted(combo_uses.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll go through the repool_libs dataframe and accumulate samples for a pool, until we hit barcode repeats. For those, we'll kick them over to a new dataframe. \n",
    "\n",
    "This way we'll end up with two pools: one that's compatible with itself, and one that we can *make* compatible with another set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repool_libs['Repool pool'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "repool_libs['Repool pool'] = np.nan\n",
    "used_combos_1 = set()\n",
    "used_combos_2 = set()\n",
    "\n",
    "for i, row in repool_libs.iterrows():\n",
    "    combo = row['index combo seq']\n",
    "\n",
    "    if combo not in used_combos_1:\n",
    "        used_combos_1.add(combo)\n",
    "        repool_libs.loc[i,'Repool pool'] = 1\n",
    "    elif combo not in used_combos_2:\n",
    "        used_combos_2.add(combo)\n",
    "        repool_libs.loc[i,'Repool pool'] = 2\n",
    "    else:\n",
    "        raise('Fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.loc[repool_libs['Repool pool'] == 1,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.loc[repool_libs['Repool pool'] == 2,].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many sequences do we need from each set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repool_libs.loc[repool_libs['Repool pool'] == 1,'Seqs below target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.loc[repool_libs['Repool pool'] == 1,'Seqs below target'].sum() / mean_seqs_per_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.loc[repool_libs['Repool pool'] == 2,'Seqs below target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs.loc[repool_libs['Repool pool'] == 2,'Seqs below target'].sum() / mean_seqs_per_lane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the major pool of barcode-compatible sequences, we have 1377 samples that together account for about 0.6 lane's worth of demand.\n",
    "\n",
    "The minor pool, at 6% of a lane, is not much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: evaluate lane splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three lanes available. Ideally, we'll sequence each sample at about twice it's expected necessary value, to assure that these all work satisfactorily. \n",
    "\n",
    "Our current splits break down as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Remade samples from plates 5-8: {0} samples, {1:4.3f} of a lane'.format(remake_5_8.shape[0], remake_5_8['Seqs below target'].sum() / mean_seqs_per_lane) )\n",
    "print('Remade samples from other lanes: {0} samples, {1:4.3f} of a lane'.format(remake_libs.shape[0], remake_libs['Seqs below target'].sum() / mean_seqs_per_lane) )\n",
    "print('Repooled samples, set 1: {0} samples, {1:4.3f} of a lane'.format(repool_libs.loc[repool_libs['Repool pool'] == 1,].shape[0], repool_libs.loc[repool_libs['Repool pool'] == 1,'Seqs below target'].sum() / mean_seqs_per_lane) )\n",
    "print('Repooled samples, set 2: {0} samples, {1:4.3f} of a lane'.format(repool_libs.loc[repool_libs['Repool pool'] == 2,].shape[0], repool_libs.loc[repool_libs['Repool pool'] == 2,'Seqs below target'].sum() / mean_seqs_per_lane) )\n",
    "print('New samples, plates 81-83: {0} samples, {1:4.3f} of a lane'.format(221, 221/384))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we want to do is to:\n",
    "1. Repool set 1 samples, put on a lane alone\n",
    "2. Repool set 2 samples, remake samples from plates 5-8 and other lanes with non-overlapping set, put on a lane together\n",
    "3. Make new samples from plates 81-83 and put on a lane alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: create new barcode file for remade plates 5-8 and new samples\n",
    "\n",
    "\n",
    "We need to remove barcode combos that have previously been used in the set 2 barcodes, and then we can use those as inputs for redone plate 5-8 and redone remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_combo_fp = './finrisk_data/Indices/temp_iTru_combos.csv'\n",
    "\n",
    "barcodes = pd.read_csv(index_combo_fp)\n",
    "\n",
    "barcodes_set2 = barcodes.loc[(barcodes['index combo seq'].isin(repool_libs.loc[repool_libs['Repool pool'] == 2,'index combo seq'])),]\n",
    "barcodes_no_set2 = barcodes.loc[~(barcodes['index combo seq'].isin(repool_libs.loc[repool_libs['Repool pool'] == 2,'index combo seq'])),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(barcodes_set2.index).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes_no_set2.to_csv('./finrisk_data/Re-pooling/barcodes_no_set2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: make repool pick list for set 1 samples\n",
    "\n",
    "This is really just the nl to pool values, set to get a volume that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs_set1 = repool_libs.loc[repool_libs['Repool pool'] == 1,]\n",
    "estimate_pool_conc_vol(repool_libs_set1['nL to target'], repool_libs_set1['Library Concentration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repool_libs_set1['Repool vol'] = np.clip(repool_libs_set1['nL to target'], 10,1000) * 2\n",
    "repool_libs_set1['Est repool seqs'] = repool_libs_set1['Repool vol'] * repool_libs_set1['Sequences per nL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_pool_conc_vol(repool_libs_set1['Repool vol'], repool_libs_set1['Library Concentration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing some wells that aren't populated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plate 13-16 didn't have Library Well value; but these are not reorganized, so 'Well' is fine\n",
    "\n",
    "repool_libs_set1.loc[repool_libs_set1['Pool name'] == 'Finrisk_13-16','Library Well'] = repool_libs_set1.loc[repool_libs_set1['Pool name'] == 'Finrisk_13-16','Well'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make pick list\n",
    "# Source Plate Name,Source Plate Type,Source Well,Concentration,Transfer Volume,Destination Plate Name,Destination Well\n",
    "\n",
    "repool_set1_picklist = repool_libs_set1.loc[:,['Pool name','Library Well','Library Concentration','Repool vol']]\n",
    "repool_set1_picklist.columns = ['Source Plate Name','Source Well','Concentration','Transfer Volume']\n",
    "repool_set1_picklist['Source Plate Type'] = '384LDV_AQ_B2_HT'\n",
    "repool_set1_picklist['Destination Plate Name'] = 'NormalizedDNA'\n",
    "repool_set1_picklist['Destination Well'] = 'A1'\n",
    "\n",
    "max_vol_per_well = 20000\n",
    "running_tot = 0\n",
    "col = 1\n",
    "for i, row in repool_set1_picklist.iterrows():\n",
    "    if running_tot >= max_vol_per_well:\n",
    "        col += 1\n",
    "        running_tot = 0\n",
    "    running_tot += row['Transfer Volume']\n",
    "    repool_set1_picklist.loc[i, 'Destination Well'] = 'A%s' % col\n",
    "\n",
    "repool_set1_picklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_set1_picklist['Transfer Volume'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write pick list\n",
    "picklist1_fp = './finrisk_data/Re-pooling/repool_set1_picklist.csv'\n",
    "repool_set1_picklist.to_csv(picklist1_fp, header=True, index=False)\n",
    "\n",
    "!head {picklist1_fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: make repool pick list for set 2 samples\n",
    "\n",
    "This is really just the nl to pool values, set to get a volume that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs_set2 = repool_libs.loc[repool_libs['Repool pool'] == 2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs_set2['nL to target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs_set2['Repool vol'] = np.clip(repool_libs_set2['nL to target'], 10,1000) * 3\n",
    "repool_libs_set2['Est repool seqs'] = repool_libs_set2['Repool vol'] * repool_libs_set2['Sequences per nL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_pool_conc_vol(repool_libs_set2['Repool vol'], repool_libs_set2['Library Concentration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make pick list\n",
    "# Source Plate Name,Source Plate Type,Source Well,Concentration,Transfer Volume,Destination Plate Name,Destination Well\n",
    "\n",
    "repool_set2_picklist = repool_libs_set2.loc[:,['Pool name','Library Well','Library Concentration','Repool vol']]\n",
    "repool_set2_picklist.columns = ['Source Plate Name','Source Well','Concentration','Transfer Volume']\n",
    "repool_set2_picklist['Source Plate Type'] = '384LDV_AQ_B2_HT'\n",
    "repool_set2_picklist['Destination Plate Name'] = 'NormalizedDNA'\n",
    "repool_set2_picklist['Destination Well'] = 'A1'\n",
    "\n",
    "max_vol_per_well = 20000\n",
    "running_tot = 0\n",
    "col = 1\n",
    "for i, row in repool_set2_picklist.iterrows():\n",
    "    if running_tot >= max_vol_per_well:\n",
    "        col += 1\n",
    "        running_tot = 0\n",
    "    running_tot += row['Transfer Volume']\n",
    "    repool_set2_picklist.loc[i, 'Destination Well'] = 'A%s' % col\n",
    "\n",
    "repool_set2_picklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_set2_picklist['Transfer Volume'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write pick list\n",
    "picklist2_fp = './finrisk_data/Re-pooling/repool_set2_picklist.csv'\n",
    "repool_set2_picklist.to_csv(picklist2_fp, header=True, index=False)\n",
    "\n",
    "!head {picklist2_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remake libraries\n",
    "\n",
    "For this we'll export the remake_libs dataframe to csv, and then import into a new complete pipeline notebook. It will require some modifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_libs.to_csv('./finrisk_data/Re-pooling/remake_libs_input_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 5-8 against new quants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_8 = lib_df.loc[lib_df['Pool name'] == 'Finrisk_05-08',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_8_newconcs = pd.read_csv('./finrisk_data/Re-pooling/11-14-17_FinRisk_5-8_minipico_inputDNA.txt', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_8[['Sample','Library Well','Sample DNA Concentration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = pd.merge(left = five_8[['Sample','Library Well','Sample DNA Concentration']],\n",
    "                right = five_8_newconcs[['Well','[Concentration]']],\n",
    "                left_on = 'Library Well',\n",
    "                right_on = 'Well')\n",
    "\n",
    "both.columns = ['Sample','Library Well','Old','Well','New']\n",
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both.plot.scatter(x = 'Old', y = 'New')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x = 'Old', y = 'New', kind=\"reg\", data=both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are different enough, going to go ahead and \n",
    "\n",
    "1. add new quant data to dataframe\n",
    "2. export with 81-83 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting new data for the combined plate 5-8 repick and plate 81-83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_5_8_df = lib_df.loc[(lib_df['Pool name'] == 'Finrisk_05-08') & (lib_df['Sample'].isin(remake_5_8['Sample'])),\n",
    "                           ['Sample','Row','Col','Blank','Library Well','Sample DNA Concentration','Pool name']]\n",
    "plate_5_8_df.rename(columns={'Library Well': 'Well'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plate_5_8_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_5_8_df.to_csv('./finrisk_data/Re-pooling/remake_5-8_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINRISK FOR JANSSEN ADENDUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j2002 = pd.read_csv('./finrisk_data/KnightLabFinrisk2002Subset.csv')\n",
    "j2002.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2007 = pd.read_csv('./finrisk_data/KnightLabFinrisk2007_DILGOM_Subset.csv')\n",
    "j2007.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get read counts for these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2002.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "j2002_stats = pd.merge(left = j2002, right = total_df[['Sample','Total Sequences']], left_on = 'Stool Barcode', right_on = 'Sample', how = 'left')\n",
    "\n",
    "\n",
    "j2002_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2002_stats['Resequencing Set'] = \"None\"\n",
    "j2002_stats.loc[j2002_stats['Sample'].isin(remake_5_8['Sample']),'Resequencing Set'] = 'Remake 5-8'\n",
    "j2002_stats.loc[j2002_stats['Sample'].isin(remake_libs['Sample']),'Resequencing Set'] = 'Remake set'\n",
    "j2002_stats.loc[j2002_stats['Sample'].isin(repool_libs_set1['Sample']),'Resequencing Set'] = 'Repool set 1'\n",
    "j2002_stats.loc[j2002_stats['Sample'].isin(repool_libs_set2['Sample']),'Resequencing Set'] = 'Repool set 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "j2002_stats['Resequencing Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2007_stats = pd.merge(left = j2007, right = total_df[['Sample','Total Sequences']], left_on = 'Stool ID', right_on = 'Sample', how = 'left')\n",
    "\n",
    "j2007_stats['Resequencing Set'] = \"None\"\n",
    "j2007_stats.loc[j2007_stats['Knight Plate'].isin(['Finrisk 2007 Plate 81',\n",
    "                                                  'Finrisk 2007 Plate 82',\n",
    "                                                  'Finrisk 2007 Plate 83']),'Resequencing Set'] = 'Plate 81-83'\n",
    "j2007_stats.loc[j2007_stats['Sample'].isin(remake_5_8['Sample']),'Resequencing Set'] = 'Remake 5-8'\n",
    "j2007_stats.loc[j2007_stats['Sample'].isin(remake_libs['Sample']),'Resequencing Set'] = 'Remake set'\n",
    "j2007_stats.loc[j2007_stats['Sample'].isin(repool_libs_set1['Sample']),'Resequencing Set'] = 'Repool set 1'\n",
    "j2007_stats.loc[j2007_stats['Sample'].isin(repool_libs_set2['Sample']),'Resequencing Set'] = 'Repool set 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2007_stats['Resequencing Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir finrisk_janssen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j2007_stats.to_csv('./finrisk_janssen/j2007_seq_status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2002_stats.to_csv('./finrisk_janssen/j2002_seq_status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2002_stats.loc[j2002_stats['Total Sequences'] > 300000,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repool_libs_set1.to_csv('./finrisk_data/Re-pooling/repool_set1_df.csv')\n",
    "repool_libs_set2.to_csv('./finrisk_data/Re-pooling/repool_set2_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
